---
title: "PLSC 30600 --- Homework 1"
output:
  pdf_document:
    number_sections: false
header-includes:
  - "\\usepackage[margin=1in]{geometry}"
  - "\\usepackage{amsmath, amssymb, amsthm}"
  - "\\usepackage{booktabs}"
  - "\\usepackage{enumitem}"
  - "\\usepackage{hyperref}"
  - "\\setlist[itemize]{noitemsep, topsep=2pt}"
  - "\\setlist[enumerate]{itemsep=4pt, topsep=6pt}"
  - "\\newcommand{\\E}{\\textrm{E}}"
---
\textbf{Due:} Fri, January 16 (11:59pm)

\vspace{0.75em}

\noindent \textbf{General instructions}
\begin{itemize}
  \item \textbf{Show your work} for all hand calculations. A final numeric answer without intermediate steps, formulas, and/or justification is not sufficient.
  \item For coding problems, submit a reproducible script \texttt{(.R/.rmd/.rnw)} along with the fully compiled pdf. Set a random seed and report it.
  \item Include a brief note describing how you used AI tools (if at all), consistent with the course AI policy.
  \item For each proof question, start with one sentence naming the technique you are using
(e.g., direct proof, Law of Iterated Expectations, bounding, counterexample, construction, contradiction).
\end{itemize}

\vspace{0.75em}
\hrule
\vspace{0.75em}


\section*{Problem 0: Proof techniques mini-lab}

For each part below: (i) state the technique you are using (e.g., bounding, counterexample, construction, Law of Iterated Expectations),
and (ii) write a clean argument. You may use the Law of Iterated Expectations without re-proving it.
Use Aronowâ€“Miller proofs as \textit{direct} templates for identification arguments; the goal is to practice working through the steps of these proofs, not to come up with new techniques. 

\begin{enumerate}[label=\textbf{0.\alph*}]
  \item \textbf{Bounding argument.}
  Let $\tilde Y_i$ be any completion of the missing outcomes such that
  $\tilde Y_i = Y_i$ when $R_i=1$ and $\tilde Y_i \in [0,1]$ when $R_i=0$.
  Let $\bar{\tilde Y} = \frac{1}{n}\sum_{i=1}^n \tilde Y_i$.
  Prove that
  \[
    \underbrace{\frac{1}{n}\sum_{i=1}^n R_i Y_i}_{\text{impute missing as }0}
    \;\le\;
    \bar{\tilde Y}
    \;\le\;
    \underbrace{\frac{1}{n}\sum_{i=1}^n \big(R_i Y_i + (1-R_i)\big)}_{\text{impute missing as }1}.
  \]
  (Interpret these two endpoints as the plug-in Manski bounds.)

  \item \textbf{Law of Iterated Expectations proof (MAR identification).}
  Assume MAR given $X$:
  \[
    Y \perp R \mid X,
    \qquad \Pr[R=1\mid X=x]>0 \text{ for each } x\in\{A,B\}.
  \]
  Prove that
  \[
    \E[Y] = \sum_{x\in\{A,B\}} \E[Y^*\mid R=1,X=x]\Pr[X=x].
  \]
  (Your proof should use the Law of Iterated Expectations and the MAR assumption at the appropriate step.)

  \item \textbf{Counterexample (independence vs conditional independence).}
  Consider binary random variables $X,Y,R\in\{0,1\}$ with $\Pr[X=0]=\Pr[X=1]=1/2$ and
  \[
  f_{Y,R\mid X}(y,r\mid x=0) =
  \begin{cases}
    1/2 & \text{if } (y,r)=(1,1),\\
    1/2 & \text{if } (y,r)=(0,0),\\
    0 & \text{otherwise},
  \end{cases}
  \qquad
  f_{Y,R\mid X}(y,r\mid x=1) =
  \begin{cases}
    1/2 & \text{if } (y,r)=(1,0),\\
    1/2 & \text{if } (y,r)=(0,1),\\
    0 & \text{otherwise}.
  \end{cases}
  \]
  \begin{enumerate}[label=(\roman*)]
    \item Show that $Y \perp R$ (unconditional independence).
    \item Show that $Y \not\!\perp R \mid X$ (conditional independence fails).
  \end{enumerate}
  Conclude: unconditional independence does \emph{not} imply conditional independence.
\end{enumerate}

\vspace{0.75em}
\hrule
\vspace{0.75em}

\section*{Setup and toy dataset (used in Problem 1 and Coding A--B)}

We observe i.i.d. draws of $(Y_i^*, R_i, X_i)$ for $i=1,\dots,n$, where:
\begin{itemize}
  \item $Y_i \in [0,1]$ is the outcome of interest (bounded).
  \item $R_i \in \{0,1\}$ is the \textbf{response/observation indicator}: $R_i=1$ means $Y_i$ is observed, $R_i=0$ means $Y_i$ is missing.
  \item $Y_i^*$ is the recorded outcome:
  \[
    Y_i^* =
    \begin{cases}
      Y_i, & R_i=1,\\
      -99, & R_i=0.
    \end{cases}
  \]
  \item $X_i \in \{A,B\}$ is a covariate observed for \textbf{all} units (including nonrespondents).
\end{itemize}

\noindent The toy dataset is:

\begin{center}
\begin{tabular}{c c c c}
\toprule
$i$ & $X_i$ & $R_i$ & $Y_i^*$ \\
\midrule
1  & A & 1 & 0.20 \\
2  & A & 0 & -99 \\
3  & B & 1 & 0.80 \\
4  & B & 0 & -99 \\
5  & A & 1 & 0.25 \\
6  & A & 0 & -99 \\
7  & B & 1 & 0.85 \\
8  & B & 1 & 0.90 \\
9  & A & 0 & -99 \\
10 & A & 1 & 0.30 \\
\bottomrule
\end{tabular}
\end{center}

\noindent Throughout, let $\mu \equiv \E[Y]$ denote the population mean of $Y$.

\vspace{0.75em}
\hrule
\vspace{0.75em}

\section*{Problem 1: Manski bounds, MCAR, and MAR on the same dataset}

\begin{enumerate}[label=\textbf{1.\alph*}]

  \item \textbf{Manski bounds.}
  Assume only that $Y\in[0,1]$. Compute the \textbf{plug-in} lower and upper bounds for $\mu=\E[Y]$ using the estimators from Aronow \& Miller Theorem 6.1.4.

  \item \textbf{MCAR (point identification).}
  Now assume \textbf{MCAR}:
  \[
    Y \perp R
    \quad \text{and} \quad
    \Pr[R=1]>0.
  \]
Compute the MCAR plug-in estimator $\widehat \mu_{\text{MCAR}}$ from the toy dataset.

  \item \textbf{MAR given $X$ (point identification by stratification).}
  Now assume \textbf{MAR given $X$}:
  \[
    Y \perp R \mid X,
    \qquad \Pr[R=1\mid X=x]>0 \ \text{for each } x\in\{A,B\}.
  \]
Compute the MAR plug-in estimator $\widehat \mu_{\text{MAR}}$ from the toy dataset.

  \item \textbf{Interpretation (short answer).}
  \begin{enumerate}[label=(\roman*)]
    \item Why is MCAR typically regarded as \textit{stronger} than MAR?
    \item How does the positivity condition differ in spirit between MCAR and MAR?
  \end{enumerate}


\item \textbf{Sanity checks via bounding (proof).}
Using the result from Problem 0(a), prove that both $\hat\mu_{\text{MCAR}}$ and $\hat\mu_{\text{MAR}}$
(from this dataset) must lie inside the plug-in Manski bounds you computed.
(Do \emph{not} argue by plugging in numbers; give an inequality argument.)
\end{enumerate}


\vspace{0.75em}
\hrule
\vspace{0.75em}



\section*{Problem 2: Propensity scores for missing data}

Let $p_R(X)\equiv \Pr[R=1\mid X]$ and assume MAR given $X$: $Y \perp R \mid X$.

\begin{enumerate}[label=\textbf{2.\alph*}]
  \item \textbf{Balance (conditioning on the propensity score).}
  Show that
  \[
    R \perp X \mid p_R(X).
  \]
  \emph{Hint:} because $R$ is binary, it suffices to show
  $\E[R\mid X, p_R(X)] = \E[R\mid p_R(X)]$.

  \item \textbf{Outcome--response independence given the propensity score.}
  Show that
  \[
    Y \perp R \mid p_R(X).
  \]
  \emph{Hint:} again use the binary-$R$ trick and the Law of Iterated Expectations; under MAR,
  $\E[R\mid Y,X]=\E[R\mid X]=p_R(X)$.
\end{enumerate}

\vspace{0.75em}
\hrule
\vspace{0.75em}

\section*{Problem 3: MCAR vs.\ MAR diagnostic thinking (vignette)}

A city surveys residents to estimate turnout in the most recent election. Let $Y\in\{0,1\}$ indicate whether a person voted, $R\in\{0,1\}$ indicate survey response, and let $X$ be age group (recorded for everyone sampled from administrative data, including nonrespondents).

The director believes:
\begin{itemize}
  \item older residents are more likely to respond (so $R$ depends on $X$),
  \item turnout differs by age (so $Y$ depends on $X$).
\end{itemize}

\begin{enumerate}[label=\textbf{3.\alph*}]
  \item Is MCAR ($Y\perp R$) plausible? Why or why not? (2--4 sentences)
  \item Give a realistic story under which MAR given age ($Y\perp R \mid X$) might be plausible. What does the story rule out? (3--5 sentences)
  \item Suppose age is \emph{only} observed for respondents (you do not observe $X$ when $R=0$). Which step in the MAR identification argument fails, and why? (3--5 sentences)
\end{enumerate}

\vspace{0.75em}
\hrule
\vspace{0.75em}

\section*{Problem 4: Potential outcomes and the basic identification gap (Lemma 1)}

Let $D\in\{0,1\}$ denote a treatment indicator. Let $Y(1)$ and $Y(0)$ be potential outcomes, and let the observed outcome be
\[
  Y = Y(1)D + Y(0)(1-D).
\]

\noindent \textbf{Lemma 1 (Identification gap).} In general,
\[
\E[Y(1)] \neq \E[Y\mid D=1]
\quad \text{and} \quad
\E[Y(0)] \neq \E[Y\mid D=0].
\]

\begin{enumerate}[label=\textbf{4.\alph*}]
  \item \textbf{Exercise: Prove Lemma 1.}
  Provide a proof by constructing a counterexample distribution for $(Y(0),Y(1),D)$ where at least one inequality holds. (You may use a finite ``science table'' counterexample or a probability model.)

  \item \textbf{When does equality hold?}
  State a sufficient condition under which
  \[
    \E[Y(1)] = \E[Y\mid D=1] \quad \text{and} \quad \E[Y(0)] = \E[Y\mid D=0].
  \]
  Briefly justify your answer (a short argument is sufficient).

  \item \textbf{Selection bias decomposition.}
  Starting from $\E[Y\mid D=1]-\E[Y\mid D=0]$, show that
  \[
    \E[Y\mid D=1]-\E[Y\mid D=0]
    = \E[Y(1)-Y(0)\mid D=1] + \Big(\E[Y(0)\mid D=1]-\E[Y(0)\mid D=0]\Big).
  \]
  Interpret the two terms in 2--4 sentences.
\end{enumerate}

\vspace{0.75em}
\hrule
\vspace{0.75em}


\section*{Problem 5: Continuous treatment and average marginal causal effects}

Let $D$ be a continuous treatment with support $[0,1]$ and suppose each unit has
potential outcomes $\{Y(d): d\in[0,1]\}$, with $Y(d)$ differentiable in $d$.
Assume strong ignorability:
\[
\{Y(d)\}_{d\in[0,1]} \perp D \mid X,
\qquad 0<\Pr[D\in \mathcal{N}\mid X] \ \text{for neighborhoods } \mathcal{N}\subset[0,1].
\]

\begin{enumerate}[label=\textbf{5.\alph*}]
  \item Define the conditional response function $m(d,x)=\E[Y\mid D=d,X=x]$.
  Show that under strong ignorability,
  \[
    \E[Y(d)] = \E\big[m(d,X)\big].
  \]

  \item Suppose
  \[
    m(d,x) = \alpha + \beta d + \gamma d^2 + \delta x + \eta (d\cdot x),
  \]
  where $x\in\{0,1\}$.
  Compute $\frac{\partial}{\partial d}m(d,x)$.

  \item The (population) average marginal causal effect (AMCE) is defined as
  \[
    \mathrm{AMCE} \equiv \E\!\left[\frac{\partial}{\partial d} m(D,X)\right].
  \]
  Assume $D\sim \mathrm{Uniform}(0,1)$ and $\Pr[X=1]=p$ with $D\perp X$.
  Compute AMCE in terms of $(\beta,\gamma,\eta,p)$.
  (Show your work, including any integrals you use.)
\end{enumerate}

\vspace{0.75em}
\hrule
\vspace{0.75em}


\section*{Coding section}

Your code should be readable and reproducible. 
You may use Claude Code for assistance, but you are responsible for correctness and interpretation.

\subsection*{Coding A: Replicate Problem 1 numerically}

\begin{enumerate}[label=\textbf{A\arabic*.}]
  \item Enter the toy dataset into your code (as a data frame with columns \texttt{i}, \texttt{X}, \texttt{R}, \texttt{Ystar}).
  \begin{verbatim}
toy <- data.frame(
  i = 1:10,
  X = c("A","A","B","B","A","A","B","B","A","A"),
  R = c(1,0,1,0,1,0,1,1,0,1),
  Ystar = c(0.20,-99,0.80,-99,0.25,-99,0.85,0.90,-99,0.30)
)


  \end{verbatim}
  \item Write a function that takes $(Y^*,R)$ and returns the plug-in Manski bounds for $\E[Y]$ when $Y\in[0,1]$.
  \item Write a function that computes $\widehat{\mu}_{\text{MCAR}}$.
  \item Write a function that computes $\widehat{\mu}_{\text{MAR}}$ by stratification on $X\in\{A,B\}$.
  \item Print the three results (bounds, MCAR estimate, MAR estimate) and verify they match your hand calculations.
\end{enumerate}

\subsection*{Coding B: Bootstrap confidence intervals}

Use the nonparametric bootstrap (resample rows with replacement).
Letting $B$ be the number of bootstrap resamples, use $B= 100$.

\begin{enumerate}[label=\textbf{B\arabic*.}]
  \item For each bootstrap sample, compute:
  \begin{itemize}
    \item the lower and upper Manski bounds;
    \item $\widehat{\mu}_{\text{MCAR}}$;
    \item $\widehat{\mu}_{\text{MAR}}$.
  \end{itemize}
  (Do \textit{not} print out all 100, just show your code.)

  \item \textbf{Positivity diagnostics in the bootstrap (MAR).}
  In some bootstrap resamples, one of the strata (A or B) may have no respondents ($R=1$), making $\widehat{\mu}_{\text{MAR}}$ undefined.
  \begin{itemize}
    \item Report the fraction of bootstrap resamples in which $\widehat{\mu}_{\text{MAR}}$ is undefined.
    \item For the resamples where it is defined, compute a 95\% percentile CI for $\mu$ under MAR.
  \end{itemize}

  \item Compute 95\% percentile bootstrap CIs for:
  \[
    \text{Lower bound},\quad \text{Upper bound},\quad \widehat{\mu}_{\text{MCAR}},\quad \widehat{\mu}_{\text{MAR}}.
  \]
\end{enumerate}